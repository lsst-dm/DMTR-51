\documentclass[DM,lsstdraft,STR,toc]{lsstdoc}
\input meta.tex

\begin{document}

\def\milestoneName{HSC Reprocessing}
\def\milestoneId{LDM-503-2}
\def\product{LSST Level 2 System}

\setDocCompact{true}

\title[\milestoneId{}~Test Report]{\milestoneId{} (\milestoneName{})~Test Report}
\setDocRef{\lsstDocType-\lsstDocNum}
\setDocDate{\vcsdate}
\setDocUpstreamLocation{\url{https://github.com/lsst/lsst-texmf/examples}}
\author{% `git log --pretty=%an | sort --key=2 | uniq` ?
  John D. Swinbank,
  Hsin-Fang Chiang,
  Michelle Gower,
  Jim Bosch
}

% Most recent last
\setDocChangeRecord{
\addtohist{1}{2017-11-30}{Initial version.}{Chiang, Gower et al.}
}


\setDocCurator{John D. Swinbank}
\setDocUpstreamLocation{\url{https://github.com/lsst-dm/\lsstDocType-\lsstDocNum}}
\setDocUpstreamVersion{\vcsrevision}


\setDocAbstract{
This is the test report for \milestoneId{} (\milestoneName{}), an LSST DM level 2 milestone pertaining to the \product{}.
}

\maketitle

\section{Introduction}
\label{sect:intro}

\subsection{Objectives}
\label{sect:objectives}

This document describes the results of tests carried out in late (calendar) 2017 on the \product{} in order to assess progress against the LSST DM level 2 milestone \milestoneId{}.
We report on the success or failure of applicable test cases and assess the state of the software and services tested.

\subsection{Scope}
\label{sect:scope}

The overall test plan for the LSST Data Management system is described in \citeds{LDM-503}.
This document specifically refers to the late (calendar) 2017 milestone \milestoneId{}, which tests the \product{}.
The overall \product{} test specification is defined in \citeds{LDM-534}.
The test plan for \milestoneId{} involves the execution of the entire DRP-00
(Small Scale Data Release Processing) specification, including the following
test cases:

\begin{description}

  \item[DRP-00-00]{Installation of the Data Release Production science payload}
  \item[DRP-00-05]{Execution of the DRP Science Payload by the Batch Production Service}
  \item[DRP-00-10]{Data Release Includes Required Data Products}
  \item[DRP-00-15]{Scientific Verification of Source Catalog}
  \item[DRP-00-20]{Scientific Verification of Forced Source Catalog}
  \item[DRP-00-25]{Scientific Verification of Object Catalog}
  \item[DRP-00-30]{Scientific Verification of Processed Visit Images}
  \item[DRP-00-35]{Scientific Verification of Coadd Images}

\end{description}

\subsection{System Overview}
\label{sect:systemoverview}

The \product{} is that part of the LSST Data Management system which will be responsible for scheduled, periodic data processing during LSST operations.
The most prominent example of such processing is the generation of LSST's annual releases of both catalog and image data.
However, the \product{} is also responsible for the generation of template images (used in the nightly processing system) and calibration products (used in both nightly and annual processing), and for the Level 2 Quality Control Service (\citeds{LDM-148}).
The \milestoneId{} milestone focuses only on the data release production part of the system.

Note that we may broadly think of the \product{} as consisting of two independent parts: the Batch Production Service, which provides scheduling and workflow services, and the Science Payloads, which contain the algorithmic content.
The \milestoneId{} milestone exercises both parts of the system.

\subsection{Applicable Documents}
\label{sect:appdocs}
\addtocounter{table}{-1}

\begin{tabular}[htb]{l l}
\citeds{LDM-294} & LSST DM Project Management Plan\\
\citeds{LDM-503} & DM Test Plan\\
\citeds{LDM-534} & \product{} Test Specification\\
\end{tabular}

\subsection{References}
\label{sect:references}

\renewcommand{\refname}{}
\bibliography{lsst,refs,books,refs_ads}

\subsection{Document Overview}
\label{sect:docoverview}

Section \ref{sect:configuration} of this document provides details of the \product{} baseline used for this test, including relevant hardware and software configurations.
Section \ref{sect:personnel} lists the individuals involved in performing the tests.
Section \ref{sect:overview} provides an overview of the test results, while Section \ref{sect:detailed} provides more detailed results from each individual test case.

\section{Test Configuration \label{sect:configuration}}
This section shall contain a full identification of the hardware and the software to which this document applies, providing
the \product  baseline.\\
Additionally, this paragraph shall also document the documents, input data description, software requirements, interface requirements and/or ICDs and coding
standards that the tests use or follow.\\
Reference to where all software used and input and output data can be retrieved, if needed again, should be provided.
\subsection{Documents \label{sect:docsconf}}
Specify the documents composing the baseline for which we are testing against. Especially it is important to provide the  issue
and revision numbers of the Software Testing Specification document.
\subsection{Software \label{sect:swconf}}
Specify the characteristics and configuration of the software that was run during the execution of the tests. This may include
system software as operating systems, compilers, etc.
\subsection{Hardware \label{sect:hwconf}}
Specify the characteristics and configuration of the hardware required to execute the tests.
\subsection{Input Data \label{sect:inputdata}}
Specify the baseline of the input data used.

%--------------------------------------------------
% PERSONNEL
%--------------------------------------------------
\section{Personnel \label{sect:personnel}}
This paragraph shall specify the personnel participating in the testing activities reported in the document and their
roles or responsibilities.

\newpage

%--------------------------------------------------
% OVERVIEW OF THE TEST RESULTS
%--------------------------------------------------
\section{Overview of the Test Results \label{sect:overview}}
\subsection{Summary Table \label{sect:summarytable}}
The following table shall summarize, among others, the completion status of each test case.\\

% The table should follow this template for the traceability scripts retrieve the information correctly
\begin{longtable} {|p{0.2\textwidth}|p{0.2\textwidth}|p{0.6\textwidth}|}\hline
{\bf TEST CASE ID} & {\bf PASS/FAIL} & {\bf COMMENTS} \\\hline
DRP-00-00 & pass/fail & comment \\\hline
DRP-00-05 & pass/fail & comment \\\hline
DRP-00-10 & pass/fail & comment \\\hline
DRP-00-15 & pass/fail & comment \\\hline
DRP-00-20 & pass/fail & comment \\\hline
DRP-00-25 & pass/fail & comment \\\hline
DRP-00-30 & pass/fail & comment \\\hline
DRP-00-35 & pass/fail & comment \\\hline
\end{longtable}


\subsection{Overall Assessment \label{sect:overallassessment}}
\begin{itemize}
\item Provide an overall assessment of the software as demonstrated by the test results in this report
\item Identify any remaining deficiencies, limitations or constraints that were detected by the testing performed
\item For each remaining deficiency, limitation or constraint describe:
\begin{itemize}
\item Its impact on software and system performance, including identification of requirements not met
\item The impact on software and system design to correct it
\item A recommended solution/approach for correcting it
\item Mantis issue
\end{itemize}
\end{itemize}

\subsection{Impact of Test Environment \label{sect:impact}}
This paragraph shall provide an assessment of the manner in which the test environment may be different from the
operational environment and the effect of this difference on the test results.

\subsection{Recommended Improvements \label{sect:recommendations}}
This section shall provide any recommended improvement in the design, operation or testing of the software tested. A discussion
of each recommendation and its impact on the software may be provided.

\begin{note}
Data Facility to provide content.
\end{note}

Operational concerns
\begin{itemize}
  \item Missing handoff process from Pipelines to Operations.
  \item Generic issues
  \begin{itemize}
    \item The workflow definition (wcl) is very fragile to any stack
	  code changes. For example, in the current task framework,
	  a config change can imply a change in the science workflow,
	  so even a seemingly simple config change can make wcl
	  outdated. With this caveat, long pipeline integration
	  time is needed to run a version of pipelines through the
	  processing framework. It is time-consuming to keep pipeline
	  workflow definitions up to date with the Stack development.
          No verification is done to ensure complete consistency.
    \item The typical usage of Butler means all filenames are
	  pre-determined in the Stack, which leads to the situation
	  that filenames are not unique and conflicts with the DBB
	  design. To customize the filenames, a Butler template
	  file is made manaully to provide a customized Butler
	  configuration file built for each processing attempt (via
	  the \texttt{desdmfw{\_}lsst{\_}plugins} pacakge; DMTN-059).
	  This manually-made Butler template needs to match the
	  workflow definition (wcl). So far this is done manually
	  and is very error-prone.  Better integration between the
	  Butler interface and the processing framework will be helpful.
    \item As our usage of Butler is not standard and well tested,
	  it is fragile to any changes in Butler implementations
	  and the integration time can be long.
    \item Inconsistent naming
          Table \ref{tab:name}
          \input{tableNaming.tex}
    \item Storing metadata to database
  \end{itemize}
  \item Infrastructure issues
  \begin{itemize}
    \item Uptime of cluster, problems with GPFS
    \item Storage space, compete with developer cluster use
    \item Busy cluster (fix: special reservations/queue)
  \end{itemize}
  \item Missing overall processing strategy
  \item Issues specific to DESDM software
  \begin{itemize}
    \item begblock scaling
    \item restart unit is per tract
    \item The cleanup of files that are no longer needed is manual and done by operator
  \end{itemize}
\end{itemize}

For LDM-503-2, some tests were carried out manually through spot checks.
Improvements and development of tools to perform the checks will be helpful.


\newpage

\section{Detailed Test Results \label{sect:detailed}}
This section describes more in detail the results of those test cases that are failed or need a more detailed revision.
\subsection{[TEST-ID]}
\subsubsection{Summary of test results}
This paragraph shall summarize the results of the test. When the completion status of the test is not 'PASS', this section
shall reference the following paragraphs for details.

\paragraph{[DRP-00-05]}
Basic data completeness and integratiry checks from LDM-534 4.2.8.3
\begin{itemize}
  \item check the existence of the expected data
  \begin{itemize}
    \item Table \ref{tab:count} listing all release data products per tract:  expected counts, actual counts, number with 0 filesizes for that datasetType
    \input{tableProductCounts}
    \item To verify the physical location of files on the filesystem
	  match the location information tracked in the archive
	  database in the Data Backbone, we used the tool
	  \texttt{compare{\_}db.py} from the \texttt{FileMgmt}
	  package.  Paths, filesize, and md5sum were compared. The
	  results were all equal for all 50656 files in tract=8766,
	  52041 files in tract=8767, and 273375 files in tract=9813.
  \end{itemize}
  \item Check existence metadata
  \begin{itemize}
    \item Table \ref{tab:metadata} shows the metadata we check for each data product type, such as visit, ccd, tract IDs.
\input{tableProductMetadata}
    \item Results of test (Non-null values in appropriate DB table for each file)
    \item DatasetType Expected Metadata num of files with NULLs for each Metadata
  \end{itemize}
  \item Check existence provenance ((shortcut) by attempt, OPM WAS GENERATED BY, OPM WAS USED BY)
  \begin{itemize}
    \item Verify each file can be linked with the step and processing attempt
          (Check for null pfw{\_}attempt{\_}id entry in desfile)
          (Check for null wgb{\_}id entry in desfile (and wgb{\_}id is in correct attempt)
    \item Verify information linking input files to each step was saved to Oracle
          (Check that at least 1 file of each input dataset type was tracked as input for each step based upon science description of pipeline - Bonus if we can check 1 vs > 1)
    \item (known products from each pipeline: https://confluence.lsstcorp.org/display/~hchiang2/Notes+on+existing+pipeline+components )
    \item Attach a full graph (graphml format) for the small tract?
  \end{itemize}
  \item Check (existence) runtime metrics
  \begin{itemize}
    \item For each tract, wallclock time vs exec totals vs overheads
    \item For each tract + executable, sum wallclock times, avg memory usage
    \input{tableRuntime.tex}
    \input{tableRuntimeExec.tex}
  \end{itemize}
\begin{note}
Data Facility to provide content.
\end{note}
\end{itemize}

\subsubsection{Problems encountered}
\paragraph{[TEST-CASE-ID]}
This paragraph shall identify by their unique identification, the test cases in which one or more problems occurred, and shall provide:
\begin{itemize}
\item A description of the problem that occurred
\item Identification of the test procedure step in which it occurred
\item Reference to the Mantis issue
\item The number of times the procedure or step was repeated in attempting to correct the problem and the outcome of each attempt (if applicable)
\item Test steps where tests were resumed for retesting (if applicable)
\end{itemize}

\paragraph{[DRP-00-05]}

\begin{itemize}
  \item During execution (LDM-534 \S 4.2.8.2), several processing
	attempts failed due to unstable GPFS availability resulted
	from hardware problems and network issues. We retried the
	attempts after the transient problems were resolved, following
	the Test Specifications.
  \item The scratch filesystem, where the prototype DBB stores the
	files, was close to full several times during test execution.
	We had to manually purge files to ensure sufficient space
	for the execution.
  \item We needed to apply a patch on top of the Stack release to
	workaround the issue that commas are part of the data IDs;
	this has been a known problem as discussed in \jira{RFC-361}.
	The fix has been agreed in RFC-365 for future implementation
	in \jira{DM-11874}, \jira{DM-11875}, and \jira{DM-11876}.
\begin{note}
Data Facility to provide content.
\end{note}
\end{itemize}

\subsubsection{Deviations from test cases/procedures}
\paragraph{[TEST-CASE-ID]}
This paragraph shall identify by their unique identification, the test cases in which one or more deviations occured and shall provide:
\begin{itemize}
\item A description of the deviation (e.g. substitution of any resource, procedural step not followed, scchedule deviation, etc)
\item The rationale of the deviation
\item An assessment of the deviation impact on the validity of the test case
\end{itemize}

\paragraph{[DRP-00-05]}
\begin{itemize}
  \item None
\begin{note}
Data Facility to provide content.
\end{note}
\end{itemize}

\end{document}
