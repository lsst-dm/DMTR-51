\documentclass[DM,lsstdraft,STR,toc]{lsstdoc}
\input meta.tex

\begin{document}

\def\milestoneName{HSC Reprocessing}
\def\milestoneId{LDM-503-2}
\def\product{LSST Level 2 System}

\setDocCompact{true}

\title[\milestoneId{}~Test Report]{\milestoneId{} (\milestoneName{})~Test Report}
\setDocRef{\lsstDocType-\lsstDocNum}
\setDocDate{\vcsdate}
\setDocUpstreamLocation{\url{https://github.com/lsst/lsst-texmf/examples}}
\author{% `git log --pretty=%an | sort --key=2 | uniq` ?
  John D. Swinbank,
  Hsin-Fang Chiang,
  Michelle Gower,
  Jim Bosch
}

% Most recent last
\setDocChangeRecord{
\addtohist{1}{2017-11-30}{Initial version.}{Chiang, Gower et al.}
}


\setDocCurator{John D. Swinbank}
\setDocUpstreamLocation{\url{https://github.com/lsst-dm/\lsstDocType-\lsstDocNum}}
\setDocUpstreamVersion{\vcsrevision}


\setDocAbstract{
This is the test report for \milestoneId{} (\milestoneName{}), an LSST DM level 2 milestone pertaining to the \product{}.
}

\maketitle

\section{Introduction}
\label{sect:intro}

\subsection{Objectives}
\label{sect:objectives}

This document describes the results of tests carried out in late (calendar) 2017 on the \product{} in order to assess progress against the LSST DM level 2 milestone \milestoneId{}.
We report on the success or failure of applicable test cases and assess the state of the software and services tested.

\subsection{Scope}
\label{sect:scope}

The overall test plan for the LSST Data Management system is described in \citeds{LDM-503}.
This document specifically refers to the late (calendar) 2017 milestone \milestoneId{}, which tests the \product{}.
The overall \product{} test specification is defined in \citeds{LDM-534}.
The test plan for \milestoneId{} involves the execution of the entire DRP-00
(Small Scale Data Release Processing) specification, including the following
test cases:

\begin{description}

  \item[DRP-00-00]{Installation of the Data Release Production science payload}
  \item[DRP-00-05]{Execution of the DRP Science Payload by the Batch Production Service}
  \item[DRP-00-10]{Data Release Includes Required Data Products}
  \item[DRP-00-15]{Scientific Verification of Source Catalog}
  \item[DRP-00-20]{Scientific Verification of Forced Source Catalog}
  \item[DRP-00-25]{Scientific Verification of Object Catalog}
  \item[DRP-00-30]{Scientific Verification of Processed Visit Images}
  \item[DRP-00-35]{Scientific Verification of Coadd Images}

\end{description}

\subsection{System Overview}
\label{sect:systemoverview}

The \product{} is that part of the LSST Data Management system which will be responsible for scheduled, periodic data processing during LSST operations.
The most prominent example of such processing is the generation of LSST's annual releases of both catalog and image data.
However, the \product{} is also responsible for the generation of template images (used in the nightly processing system) and calibration products (used in both nightly and annual processing), and for the Level 2 Quality Control Service (\citeds{LDM-148}).
The \milestoneId{} milestone focuses only on the data release production part of the system.

Note that we may broadly think of the \product{} as consisting of two independent parts: the Batch Production Service, which provides scheduling and workflow services, and the Science Payloads, which contain the algorithmic content.
The \milestoneId{} milestone exercises both parts of the system.

\subsection{Applicable Documents}
\label{sect:appdocs}
\addtocounter{table}{-1}

\begin{tabular}[htb]{l l}
\citeds{LDM-294} & LSST DM Project Management Plan\\
\citeds{LDM-503} & DM Test Plan\\
\citeds{LDM-534} & \product{} Test Specification\\
\end{tabular}

\subsection{References}
\label{sect:references}

\renewcommand{\refname}{}
\bibliography{lsst,refs,books,refs_ads}

\subsection{Document Overview}
\label{sect:docoverview}

Section \ref{sect:configuration} of this document provides details of the \product{} baseline used for this test, including relevant hardware and software configurations.
Section \ref{sect:personnel} lists the individuals involved in performing the tests.
Section \ref{sect:overview} provides an overview of the test results, while Section \ref{sect:detailed} provides more detailed results from each individual test case.

\section{Test Configuration \label{sect:configuration}}
This section shall contain a full identification of the hardware and the software to which this document applies, providing
the \product  baseline.\\
Additionally, this paragraph shall also document the documents, input data description, software requirements, interface requirements and/or ICDs and coding
standards that the tests use or follow.\\
Reference to where all software used and input and output data can be retrieved, if needed again, should be provided.
\subsection{Documents \label{sect:docsconf}}
Specify the documents composing the baseline for which we are testing against. Especially it is important to provide the  issue
and revision numbers of the Software Testing Specification document.
\subsection{Software \label{sect:swconf}}
Specify the characteristics and configuration of the software that was run during the execution of the tests. This may include
system software as operating systems, compilers, etc.
\subsection{Hardware \label{sect:hwconf}}
Specify the characteristics and configuration of the hardware required to execute the tests.
\subsection{Input Data \label{sect:inputdata}}
Specify the baseline of the input data used.

\section{Personnel}
\label{sect:personnel}

Test case DRP-00-00 was executed by John Swinbank (University of Washington).

Test case DRP-00-05 was executed by Hsin-Fang Chiang (NCSA), Michelle Butler (NCSA), Mikolaj Kowalik (NCSA), Greg Daues (NCSA) and Rob Kooper (NCSA).

Test cases DRP-00-10, DRP-00-15, DRP-00-20, DRP-00-25, DRP-00-30 and DRP-00-35 were executed by Jim Bosch (Princeton), Lauren MacArthur (Princeton) and Tim Morton (Princeton).

\newpage

\section{Overview of the Test Results}
\label{sect:overview}

\subsection{Summary Table}
\label{sect:summarytable}

\begin{longtable} {|p{0.2\textwidth}|p{0.2\textwidth}|p{0.6\textwidth}|}\hline
{\bf TEST CASE ID} & {\bf PASS/FAIL} & {\bf COMMENTS} \\\hline
DRP-00-00 & pass/fail & comment \\\hline
DRP-00-05 & pass & Refer to \S\ref{sect:detailed}. \\\hline
DRP-00-10 & pass/fail & comment \\\hline
DRP-00-15 & pass/fail & comment \\\hline
DRP-00-20 & pass/fail & comment \\\hline
DRP-00-25 & pass/fail & comment \\\hline
DRP-00-30 & pass/fail & comment \\\hline
DRP-00-35 & pass/fail & comment \\\hline
\end{longtable}

\subsection{Overall Assessment \label{sect:overallassessment}}
\begin{itemize}
\item Provide an overall assessment of the software as demonstrated by the test results in this report
\item Identify any remaining deficiencies, limitations or constraints that were detected by the testing performed
\item For each remaining deficiency, limitation or constraint describe:
\begin{itemize}
\item Its impact on software and system performance, including identification of requirements not met
\item The impact on software and system design to correct it
\item A recommended solution/approach for correcting it
\item Mantis issue
\end{itemize}
\end{itemize}

\subsection{Impact of Test Environment \label{sect:impact}}
This paragraph shall provide an assessment of the manner in which the test environment may be different from the
operational environment and the effect of this difference on the test results.

\subsection{Recommended Improvements \label{sect:recommendations}}
This section shall provide any recommended improvement in the design, operation or testing of the software tested. A discussion
of each recommendation and its impact on the software may be provided.

\begin{note}
Data Facility to provide content.
\end{note}

\subsubsection{DRP-00-05}

\paragraph{Operational Concerns}

\begin{itemize}

  \item{
    For \milestoneId{}, some tests were carried out manually through spot checks.
    Further development of verification tools to perform these and additional checks is needed.
  }

  \item{
    The handoff process from Pipelines to Operations needs to be improved.
    Improvements include providing details about changes in the Pipeline.
    Currently the Operator is digging through \texttt{pipe{\_}drivers} code to figure out some of these details in addition to repeatedly consulting pipeline developers.
  Providing change details during handoff will not only speed up configuring and executing the pipeline, but will also provide the information needed to configure the verification tests themselves.
    Operations should provide a first proposal on what information they would like provided.
  }

  \item{
    Making what can seem like minor changes to the pipeline definition can require more integration time.
    Examples of pipeline changes that affect the pipeline execution configuration and verification tests:

    \begin{itemize}

      \item{Pipeline code changes or task configuration changes can alter the required input files or what output files are created.}
      \item{Changing what pipeline steps should be executed may require a pipeline task configuration change in addition to modifying the overall pipeline definition.}

    \end{itemize}

    One of the goals of the new Butler and SuperTask effort is to minimize the amount of integration effort.
    When running the exact same configuration as the developer, the new Butler and SuperTask effort could definitely help.
    However, it is not yet clear how much integration/configuration effort will still be required with the new Butler+SuperTask if the operator wishes to run something different than the default pipeline configuration.
  }

  \item{
    At the time of this test, it is not yet understood exactly how Operations would configure the execution of the pipeline for best scalability and management.
    This execution may not match small-scale developer tests where getting individual results faster is more important.
    The desire is to have the test match the expected release campaign execution configuration
  as much as possible.
    As work is done on both the pipeline science and the Batch Processing Service, effort should be undertaken to determine the release campaign execution configuration.
  }

\end{itemize}

\paragraph{Infrastructure Issues}

During LDM-503-2 pipeline integration and actual test execution, there were multiple infrastructure issues with the Verification Cluster.

\begin{itemize}

  \item{
    The Verification Cluster had multiple outages due to networking equipment.
    The Network group replaced the broken routers and purchased a hot spare.
  }

  \item{
    The Verification Cluster can be very busy making it difficult to acquire nodes to do the test work (both initial configuration test and actual full test).
    To support production processing, production reservations were added to Slurm, and production
queues are planned in the future.
  }


  \item{
    This test used the scratch filesystem for both the Data Backbone as well as the job scratch.
    This filesystem is regularly full preventing the running of the tests.
    The system admins turned on the purging of the scratch filesystem, and production filesystems are being purchased and installed in the next cycle as described in the Aquisition Strategy Document \textbf{CITATION NEEDED}.
  }

\end{itemize}

\paragraph{Butler Issues}

It is expected that the new version of the Butler should solve all of these issues.

\begin{itemize}

  \item{
    The typical usage of Butler means all filenames are pre-determined in the Stack, which leads to the situation that filenames are not unique and conflicts with the
    Data Backbone design.
    To customize the filenames, a Butler template file is made manaully to provide a customized Butler configuration file built for each processing attempt (via the \texttt{desdmfw{\_}lsst{\_}plugins} package; DMTN-059 \textbf{CITATION}).

    Keeping this customized Butler template file up to date with the current Butler and Pipeline definitions is done manually and can be very error prone especially without the proper change details provided during the handoff between the Pipeline and Butler developers and Operations.
    This means that integration of changes to the current Butler or new pipeline definitions will take longer delaying the execution of a test or production campaign.
  }

  \item{
    As the ``walled-garden'' usage of Butler is not standard, it is fragile to any changes in Butler implementations.
    This lengthens pipeline integration time.
  }

  \item{
    For the purposes of this test, retrieving metadata from files to store in the database is currently limited to information known by the processing framework (e.g., visit, tract, patch, etc.).
    While the DESDM software allows specialized plugins to be written to get the metadata from the file, a seperate plugin using LSST stack methods may be required for each types of files (i.e., it is not just a matter of using pyfits to read fits headers nor is there a single I/O layer function that hides the file format details).
    The new Butler implementation should fix this.
  }

\end{itemize}

\paragraph{Issues encountered that are specific to the use of DESDM}

\begin{itemize}
  \item{
    The step in DESDM framework which configures each step for each compute job does not scale well at the size of the larger tract.
    In DESDM Operations the unit of work in a single submission is smaller, so this has not been an issue there.
    For short term use, this code could be profiled to see if there are straight forward ways to speed it up.
    Long term decisions about completely rewriting this code depends upon the 2018 decision about whether continuing to use DESDM software.
  }

  \item{
    In DESDM Operations, the single submission's unit of work is chosen to be the amount of work the Operator is willing to start over from the beginning.
    In this test, this unit of work is tract using the current operational configuration of the pipeline.
    Restarting a tract from the very beginning adds a lot of extra compute time and resources to complete the test (especially when including the scaling issue
mentioned in the previous item).
    Short term solutions include modifying operational configuration of the pipeline to make smaller units of work (which currently means human overhead in managing more submissions).
    Also, DESDM allows for manually starting a pipeline execution using outputs in the Data Backbone from a previous submission.
    But this requires an operational pipeline configuration change which takes human knowledge, effort and time.
    Long term decisions about helping to automate restarts depends upon the 2018 decision about whether the DESDM software will continue to be used for LSST processing.
  }

  \item{
    The workflow definition (wcl) that describes a pipeline to the DESDM framework includes how to name output files.
    These naming patterns need to match the manually-made Butler policies.
    For this test, this was done manually and was very error-prone.
    Until a solution comes from the Batch Processing Service using the new Butler and SuperTask, modifying the DESDM framework to automatically create the Butler policy file from the pipeline definition would speed up the integration process.
  }

\end{itemize}

\newpage

\section{Detailed Test Results}
\label{sect:detailed}

\paragraph{DRP-00-05}

\begin{enumerate}

  \item{Check the existence of the expected files: PASSED}

  \begin{enumerate}

    \item{
      Table \ref{tab:count} list all release data products per tract:  expected counts, actual counts, number with 0 filesizes for each dataset type.}

    \item{
      To verify the physical location of files on the filesystem match the location information tracked in the Data Backbone database tables, we used the tool
	  \texttt{compare{\_}db.py} from the DESDM \texttt{FileMgmt} package.
      Paths, file sizes, and checksums (MD5) were compared.
      The test results were that both the database and filesystem matched with 50656 files in tract 8766, 52041 files in tract 8767, and 273375 files in tract 9813.
    }
  \end{enumerate}

  \item{
    Check existence of the expected metadata: PASSED\\
    The following metadata is expected to have been saved

    \begin{itemize}
      \item{calexp: tract, visit, filter, ccd}
      \item{deepCoadd\_calexp: tract, patch, filter}
    \end{itemize}

    It was verified that the above mentioned metadata had non-NULL values stored for the data products in the Data Backbone database tables.
  }

  \item{
    Check existence of the expected provenance: PASSED

    \begin{enumerate}
      \item{Table \ref{tab:provenance} shows that each file can be linked with the step and processing attempt.}
      \item{Via manual spot checks, it was verified that information linking input files to each step was saved to the Data Backbone database tables.}
    \end{enumerate}
  }

  \item{Check (existence) runtime metrics: PASSED

    \begin{enumerate}
      \item{Table \ref{tab:runtime} shows wallclock time for running the entire pipeline for each tract and total cpu time per execution.
      \item Table \ref{tab:runtimeExec} shows for each tract + pipeline step:  number of executions, total user time, total system time, and largest MAXRSS.}
    \end{enumerate}
  }
\end{enumerate}

\subsubsection{Problems encountered}
\paragraph{[TEST-CASE-ID]}
This paragraph shall identify by their unique identification, the test cases in which one or more problems occurred, and shall provide:
\begin{itemize}
\item A description of the problem that occurred
\item Identification of the test procedure step in which it occurred
\item Reference to the Mantis issue
\item The number of times the procedure or step was repeated in attempting to correct the problem and the outcome of each attempt (if applicable)
\item Test steps where tests were resumed for retesting (if applicable)
\end{itemize}

\paragraph{DRP-00-05}

\begin{itemize}

  \item{
    During execution (LDM-534 \S 4.2.8.2), several processing attempts failed due to unstable GPFS availability resulted from hardware problems and network issues.
    We retried the attempts after the transient problems were resolved, following the Test Specifications.
  }

  \item{
    The scratch filesystem, where the prototype DBB stores the files, was close to full several times during test execution.
	We had to manually purge files to ensure sufficient space for the execution.
  }

  \item{
    We needed to apply a patch on top of the Stack release to workaround the issue that commas are part of the data IDs; this has been a known problem as discussed in \jira{RFC-361}.
	The fix has been agreed in RFC-365 for future implementation in \jira{DM-11874}, \jira{DM-11875}, and \jira{DM-11876}.
  }

\end{itemize}

\subsubsection{Deviations from test cases/procedures}
\paragraph{[TEST-CASE-ID]}
This paragraph shall identify by their unique identification, the test cases in which one or more deviations occured and shall provide:
\begin{itemize}
\item A description of the deviation (e.g. substitution of any resource, procedural step not followed, scchedule deviation, etc)
\item The rationale of the deviation
\item An assessment of the deviation impact on the validity of the test case
\end{itemize}

\paragraph{DRP-00-05}
\label{sec:deviation-drp-00-05}

\begin{itemize}
  \item{
    For the first processing attempts of tracts 8766 and 8767, there was a misconfiguration which executed both attempts on the same set of worker nodes simultaneously.
    This does not match the procedure in the Test Specification.
    While tract 8766 finished successfully, it took longer wallclock time than expected.
    Because it produced expected outputs, tract 8766 was not rerun.
    The first attempt of tract 8767 failed due to a network problem, and the time reported in Table \ref{tab:runtime} is the result of a new attempt (based on the Test Specification regarding resubmission of intermittent infrastructure problems) configured correctly using non-shared nodes.
    Thus, the wallclock time for tract 8767 in Table \ref{tab:runtime} is closer to the expected
	time.
  }
\end{itemize}

\appendix

\newpage
\section{Detailed tests results \label{sect:results}}

\input{tableProductCounts}
\input{tableZeroFiles}
\input{tableProvenance}
\input{tableRuntime}
\input{tableRuntimeExec}

\end{document}
