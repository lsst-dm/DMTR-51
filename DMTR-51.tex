\documentclass[DM,lsstdraft,STR,toc]{lsstdoc}

\begin{document}

%set the WP number or product here for the requirements
\def\product{LSST Level 2 System}

\setDocCompact{true}

%Product name first in title
\title    [TR for \product]
                    {\color[rgb]{0.16,0.42,0.57} \sf \product ~ Test Report}
\setDocRef      {TR-XXXXXX} % the reference code
\setDocDate     {\today}              % the date of the issue
\setDocUpstreamLocation{\url{https://github.com/lsst/lsst-texmf/examples}}

\author   {}

% Comment out the setLSSTDU if you do not want it do not set it to Blanck

%
% a short abstract
%
\setDocAbstract {This is a template for the Test Report for \product. It renders findings and results
from test controls.}

%
% the title page
%
\maketitle

%
%	Revision history MOST RECENT FIRST
%

\setDocChangeRecord{%
\addtohist{1.0}{2017-11-30}{Initial version.} {}
}


\section{Introduction \label{sect:intro}}

\subsection{Objectives \label{sect:objectives}}
This section shall describe the objectives of the report, viz. document the testing activities that were carried out against the STP and STS, assessment of the software tested, etc.

\subsection{Scope \label{sect:scope}}
The general ambit of \product testing is defined in \citell{LDM-503}.\\ Specific scope of the test campaign that this report documents
should be described in this section.

\subsection{System Overview \label{sect:systemoverview}}
This paragraph shall briefly states the purpose of the system and the software to which this document applies.
It shall describe the general nature of the system and software; summarize the history of system development, operation
and maintenance.
A summary of the software functionality,  software configuration , its  operational environment and its external interfaces can be provided.

\subsection{Applicable Documents \label{sect:appdocs}}
\addtocounter{table}{-1}

\begin{tabular}[htb]{l l}
\citeds{LDM-534} & \product Testing Specification\\
\citeds{LDM-503}& DM Test Plan\\
\citeds{LDM-294}& LSST DM Project Management Plan\\
\end{tabular}

\subsection{References\label{sect:references}}

\renewcommand{\refname}{}
\bibliography{lsst,refs,books,refs_ads}

\subsection{Definitions, acronyms, and abbreviations \label{sect:acronyms}} % include acronyms.tex generated by the acronyms.csh (GaiaTools)
%\input{acronyms}

\subsection{Document Overview \label{sect:docoverview}}
This section shall describe the contents of the document and explains how the rest of the report is organized.

%----------------------------------------------------
% TEST CONFIGURATION - baseline
%----------------------------------------------------
\section{Test Configuration \label{sect:configuration}}
This section shall contain a full identification of the hardware and the software to which this document applies, providing
the \product  baseline.\\
Additionally, this paragraph shall also document the documents, input data description, software requirements, interface requirements and/or ICDs and coding
standards that the tests use or follow.\\
Reference to where all software used and input and output data can be retrieved, if needed again, should be provided.
\subsection{Documents \label{sect:docsconf}}
Specify the documents composing the baseline for which we are testing against. Especially it is important to provide the  issue
and revision numbers of the Software Testing Specification document.
\subsection{Software \label{sect:swconf}}
Specify the characteristics and configuration of the software that was run during the execution of the tests. This may include
system software as operating systems, compilers, etc.
\subsection{Hardware \label{sect:hwconf}}
Specify the characteristics and configuration of the hardware required to execute the tests.
\subsection{Input Data \label{sect:inputdata}}
Specify the baseline of the input data used.

%--------------------------------------------------
% PERSONNEL
%--------------------------------------------------
\section{Personnel \label{sect:personnel}}
This paragraph shall specify the personnel participating in the testing activities reported in the document and their
roles or responsibilities.

\newpage

%--------------------------------------------------
% OVERVIEW OF THE TEST RESULTS
%--------------------------------------------------
\section{Overview of the Test Results \label{sect:overview}}
\subsection{Summary Table \label{sect:summarytable}}
The following table shall summarize, among others, the completion status of each test case.\\

% The table should follow this template for the traceability scripts retrieve the information correctly
\begin{longtable} {|p{0.2\textwidth}|p{0.2\textwidth}|p{0.6\textwidth}|}\hline
{\bf TEST CASE ID} & {\bf PASS/FAIL} & {\bf COMMENTS} \\\hline
DRP-00-00 & pass/fail & comment \\\hline
DRP-00-05 & pass/fail & comment \\\hline
DRP-00-10 & pass/fail & comment \\\hline
DRP-00-15 & pass/fail & comment \\\hline
DRP-00-20 & pass/fail & comment \\\hline
DRP-00-25 & pass/fail & comment \\\hline
DRP-00-30 & pass/fail & comment \\\hline
DRP-00-35 & pass/fail & comment \\\hline
\end{longtable}


\subsection{Overall Assessment \label{sect:overallassessment}}
\begin{itemize}
\item Provide an overall assessment of the software as demonstrated by the test results in this report
\item Identify any remaining deficiencies, limitations or constraints that were detected by the testing performed
\item For each remaining deficiency, limitation or constraint describe:
\begin{itemize}
\item Its impact on software and system performance, including identification of requirements not met
\item The impact on software and system design to correct it
\item A recommended solution/approach for correcting it
\item Mantis issue
\end{itemize}
\end{itemize}

\subsection{Impact of Test Environment \label{sect:impact}}
This paragraph shall provide an assessment of the manner in which the test environment may be different from the
operational environment and the effect of this difference on the test results.

\subsection{Recommended Improvements \label{sect:recommendations}}
This section shall provide any recommended improvement in the design, operation or testing of the software tested. A discussion
of each recommendation and its impact on the software may be provided.

\begin{note}
Data Facility to provide content.
\end{note}

\subsubsection{DRP-00-05}
\begin{itemize}
\item Operational concerns
\begin{itemize}
  \item For LDM-503-2, some tests were carried out manually through 
  spot checks.  Further development of verification tools to perform
  these and additional checks is needed.
  \item The handoff process from Pipelines to Operations needs to
  be improved.  Improvements include providing details about changes
  in the Pipeline.   Currently the Operator is digging through
  \texttt{pipe{\_}drivers} code to figure out some of these details 
  in addition to repeatedly consulting pipeline developers.
  Providing change details during handoff will not only
  speed up configuring and executing the pipeline, but will also
  provide the information needed to configure the verification tests
  themselves.  Operations should provide a first proposal on what
  information they would like provided.

  \item Making what can seem like minor changes to the pipeline
  definition can require more integration time.  Some examples of 
  pipeline changes that affect the pipeline 
  execution configuration and verification tests:
  \begin{itemize}
  \item Some pipeline task configuration changes alter the
      required input files or what output files are created.  
  \item Changing what pipeline steps should be executed 
      may require a pipeline task configuration change in
      addition to modifying the overall pipeline definition.
  \end{itemize}
  One of the goals of the new Butler and SuperTask effort is
  to minimize the amount of integration effort. When running 
  the exact same configuration as the developer, the new 
  Butler and SuperTask effort could 
  definitely help.   However, it is not yet clear how much
  integration/configuration effort will still be required with the
  new Butler+SuperTask if the test/Operator wishes to run 
  something different than the default pipeline configuration.

  \item At the time of LDM-503-2, it is not yet understood exactly
  how Operations would configure the execution of the pipeline for 
  best scalability and management.  This execution may not match 
  small-scale developer tests where getting individual results 
  faster is more important.  The desire is to have the
  test match the expected release campaign execution configuration
  as much as possible.   As work is done on both the pipeline 
  science and the Batch Processing Service, effort should be
  undertaken to determine the release campaign execution
  configuration.
\end{itemize}
  \item Infrastructure issues \\
  During LDM-503-2 pipeline integration and actual test execution,
  there were multiple infrastructure issues with the Verification
  Cluster.
  \begin{itemize}
    \item The Verification Cluster had multiple outages due to
networking equipment.    The Network group replaced the
broken routers and purchased a hot spare.
    \item The Verification Cluster can be very busy making it
difficult to acquire nodes to do the test work (both initial
configuration test and actual full test).  To support production 
processing, special reservations were added to Slurm, and special 
queues are planned in the future.
    \item For LDM-503-2, the test used the scratch filesystem
for both the Data Backbone as well as the job scratch.  This
filesystem is regularly full preventing the running of the 
tests.   The system admins turned on the purging of the scratch 
filesystem, and production filesystems are being purchased
and installed in the next cycle as described in the Aquisition 
Strategy Document. 
  \end{itemize}

  \item Butler issues \\
    It is expected that the new version of the Butler should
    solve all of these issues.
    \begin{itemize}
    \item The typical usage of Butler means all filenames are
	  pre-determined in the Stack, which leads to the situation
	  that filenames are not unique and conflicts with the 
	  Data Backbone design.   To customize the filenames, 
      a Butler template file is made manaully to provide a 
      customized Butler configuration file built for each 
      processing attempt (via
	  the \texttt{desdmfw{\_}lsst{\_}plugins} package; DMTN-059).

      Keeping this customized Butler template file up to date
      with the current Butler and Pipeline definitions is
      done manually and can be very error prone especially
      without the proper change details provided during the
      handoff between the Pipeline and Butler developers and
      Operations.   This means that integration of changes 
      to the current Butler or
      new pipeline definitions will take longer delaying the
      execution of a test or production campaign.

    \item As the "walled-garden" usage of Butler is not standard,
	  it is fragile to any changes in Butler implementations.
      This lengthens pipeline integration time.

    \item For LDM-503-2, retrieving metadata from files to store in
the database is currently limited to information known by the 
processing framework (e.g., visit, tract, patch, etc.).  While the
DESDM software allows specialized plugins to be written to get
the metadata from the file, there does not seem to be a consistent
LSST stack method to use across different types of files (i.e., 
it is not just a matter of using pyfits to read fits headers nor
is there a single I/O layer function that hides the file format
details).   The new Butler implementation should fix this. 
  \end{itemize}
  \item Issues encountered that are specific to the use of DESDM
framework
  \begin{itemize}
    \item The step in DESDM which configures each step for each compute job does
not scale well at the size of the larger tract.  In DESDM the unit of
work in a single submission is smaller, so this has not been an issue
there.   For short term use, this code could be profiled to see if
there are straight forward ways to speed it up.   Long term decisions
about completely rewriting this code depends upon the 2018 decision 
about whether continuing to use DESDM software.  

    \item In DESDM, the single submission's unit of work is chosen to
be the amount of work the Operator is willing to start over from
the beginning.   In the current operational configuration of the
pipeline, this unit of work is tract.   Restarting a tract from the
very beginning adds a lot of extra compute time and resources to 
complete the test (especially when including the scaling issue
mentioned in the previous item).   Short term solutions include
modifying operational configuration of the pipeline to make smaller 
units of work (which currently means human overhead in managing
more submissions).   Also, DESDM allows for manually starting a 
pipeline execution using outputs in the Data Backbone from a 
previous submission.   But this requires an operational 
pipeline configuration change which takes human knowledge, effort
and time.   Long term decisions about helping to automate restarts
depends upon the 2018 decision about whether continuing to use DESDM 
software.
	\item The workflow definition (wcl) that describes a pipeline
      to the DESDM framework includes how to name output files.
      These naming patterns need to match the manually-made Butler 
      policies.  For LDM-503-2 this is done manually
	  and can be very error-prone.  Until a solution comes from
      the Batch Processing Service using the new Butler and SuperTask,
      modifying the DESDM framework to automatically create the Butler 
      policy file from the pipeline definition would speed up the 
      integration process. 
  \end{itemize}
\end{itemize}

\newpage

\section{Detailed Test Results \label{sect:detailed}}
This section describes more in detail the results of those test cases that are failed or need a more detailed revision.
\subsection{[TEST-ID]}
\subsubsection{Summary of test results}
This paragraph shall summarize the results of the test. When the completion status of the test is not 'PASS', this section
shall reference the following paragraphs for details.

\paragraph{[DRP-00-05]}
Basic data completeness and integratiry checks from LDM-534 4.2.8.3
\begin{itemize}
  \item check the existence of the expected data
  \begin{itemize}
    \item Table \ref{tab:count} listing all release data products per tract:  expected counts, actual counts, number with 0 filesizes for that datasetType
    \item To verify the physical location of files on the filesystem
	  match the location information tracked in the archive
	  database in the Data Backbone, we used the tool
	  \texttt{compare{\_}db.py} from the \texttt{FileMgmt}
	  package.  Paths, filesize, and md5sum were compared. The
	  results were all equal for all 50656 files in tract=8766,
	  52041 files in tract=8767, and 273375 files in tract=9813.
  \end{itemize}
  \item Check existence metadata
  \begin{itemize}
    \item Table \ref{tab:metadata} shows the metadata we check for each data product type, such as visit, ccd, tract IDs.
\input{tableProductMetadata}
    \item Results of test (Non-null values in appropriate DB table for each file)
    \item DatasetType Expected Metadata num of files with NULLs for each Metadata
  \end{itemize}
  \item Check existence provenance ((shortcut) by attempt, OPM WAS GENERATED BY, OPM WAS USED BY)
  \begin{itemize}
    \item Verify each file can be linked with the step and processing attempt
          (Check for null pfw{\_}attempt{\_}id entry in desfile)
          (Check for null wgb{\_}id entry in desfile (and wgb{\_}id is in correct attempt)
    \item Verify information linking input files to each step was saved to Oracle
          (Check that at least 1 file of each input dataset type was tracked as input for each step based upon science description of pipeline - Bonus if we can check 1 vs > 1)
    \item (known products from each pipeline: https://confluence.lsstcorp.org/display/~hchiang2/Notes+on+existing+pipeline+components )
    \item Attach a full graph (graphml format) for the small tract?
  \end{itemize}
  \item Check (existence) runtime metrics
  \begin{itemize}
    \item For each tract, wallclock time vs exec totals vs overheads
    \item For each tract + executable, sum wallclock times, avg memory usage
    \input{tableRuntime.tex}
    \input{tableRuntimeExec.tex}
  \end{itemize}
\begin{note}
Data Facility to provide content.
\end{note}
\end{itemize}

\input{tableProductCounts}
\input{tableZeroFiles}
\input{tableProvenance}

\subsubsection{Problems encountered}
\paragraph{[TEST-CASE-ID]}
This paragraph shall identify by their unique identification, the test cases in which one or more problems occurred, and shall provide:
\begin{itemize}
\item A description of the problem that occurred
\item Identification of the test procedure step in which it occurred
\item Reference to the Mantis issue
\item The number of times the procedure or step was repeated in attempting to correct the problem and the outcome of each attempt (if applicable)
\item Test steps where tests were resumed for retesting (if applicable)
\end{itemize}

\paragraph{[DRP-00-05]}

\begin{itemize}
  \item During execution (LDM-534 \S 4.2.8.2), several processing
	attempts failed due to unstable GPFS availability resulted
	from hardware problems and network issues. We retried the
	attempts after the transient problems were resolved, following
	the Test Specifications.
  \item The scratch filesystem, where the prototype DBB stores the
	files, was close to full several times during test execution.
	We had to manually purge files to ensure sufficient space
	for the execution.
  \item We needed to apply a patch on top of the Stack release to
	workaround the issue that commas are part of the data IDs;
	this has been a known problem as discussed in \jira{RFC-361}.
	The fix has been agreed in RFC-365 for future implementation
	in \jira{DM-11874}, \jira{DM-11875}, and \jira{DM-11876}.
\begin{note}
Data Facility to provide content.
\end{note}
\end{itemize}

\subsubsection{Deviations from test cases/procedures}
\paragraph{[TEST-CASE-ID]}
This paragraph shall identify by their unique identification, the test cases in which one or more deviations occured and shall provide:
\begin{itemize}
\item A description of the deviation (e.g. substitution of any resource, procedural step not followed, scchedule deviation, etc)
\item The rationale of the deviation
\item An assessment of the deviation impact on the validity of the test case
\end{itemize}

\paragraph{[DRP-00-05]}
\begin{itemize}
  \item None
\begin{note}
Data Facility to provide content.
\end{note}
\end{itemize}

\newpage

\section{Test Review Board Declaration\label{sect:trb}}
This section must be filled only if the test campaign has a  formal
test review board.
After the tests are executed and the test team circulates a draft of
this document the TRB meets to declare the completion of the test campaign.
This section must contain this declaration and the pass of fail status
of the tests after completion (in case of failing it should also account
for the actions taken - Mantis issues, etc.)

\newpage

\appendix
\section{ANNEX A: Unit Tests \label{sect:unit_tests}}
Description of the unit test coverage given by Cobertura

\newpage
\section{ANNEX B: Patch releases verification}

{\it describe here the test activities executed in order to garantee the quality of the patch release}

\subsubsection{Patch release C.M.n}

Due to the small number issues fixed with the {\it \product} patch release {\it C.M.n} (very short jira list)
and due to the minimal changes applied to the software, the test campaign completed for the {\it \product} major
release $C.M$ is still valid.

All unit and integration tests are still running without problems.

Following manual verification has been done:

\begin{itemize}
\item verification 1
\item ...
\end{itemize}

No system test has been re-executed, but the fix has been verified with a specific run
on {\it Enclave??} platform.

\end{document}
