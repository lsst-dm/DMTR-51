\documentclass[DM,lsstdraft,STR,toc]{lsstdoc}

\begin{document}

%set the WP number or product here for the requirements
\def\product{LSST Level 2 System}

\setDocCompact{true}

%Product name first in title
\title    [TR for \product]
                    {\color[rgb]{0.16,0.42,0.57} \sf \product ~ Test Report}
\setDocRef      {TR-XXXXXX} % the reference code
\setDocDate     {\today}              % the date of the issue
\setDocUpstreamLocation{\url{https://github.com/lsst/lsst-texmf/examples}}

\author   {}

% Comment out the setLSSTDU if you do not want it do not set it to Blanck

%
% a short abstract
%
\setDocAbstract {This is a template for the Test Report for \product. It renders findings and results
from test controls.}

%
% the title page
%
\maketitle

%
%	Revision history MOST RECENT FIRST
%

\setDocChangeRecord{%
\addtohist{1.0}{2017-11-30}{Initial version.} {}
}


\section{Introduction \label{sect:intro}}

\subsection{Objectives \label{sect:objectives}}
This section shall describe the objectives of the report, viz. document the testing activities that were carried out against the STP and STS, assessment of the software tested, etc.

\subsection{Scope \label{sect:scope}}
The general ambit of \product testing is defined in \citell{LDM-503}.\\ Specific scope of the test campaign that this report documents
should be described in this section.

\subsection{System Overview \label{sect:systemoverview}}
This paragraph shall briefly states the purpose of the system and the software to which this document applies.
It shall describe the general nature of the system and software; summarize the history of system development, operation
and maintenance.
A summary of the software functionality,  software configuration , its  operational environment and its external interfaces can be provided.

\subsection{Applicable Documents \label{sect:appdocs}}
\addtocounter{table}{-1}

\begin{tabular}[htb]{l l}
\citeds{LDM-534} & \product Testing Specification\\
\citeds{LDM-503}& DM Test Plan\\
\citeds{LDM-294}& LSST DM Project Management Plan\\
\end{tabular}

\subsection{References\label{sect:references}}

\renewcommand{\refname}{}
\bibliography{lsst,refs,books,refs_ads}

\subsection{Definitions, acronyms, and abbreviations \label{sect:acronyms}} % include acronyms.tex generated by the acronyms.csh (GaiaTools)
%\input{acronyms}

\subsection{Document Overview \label{sect:docoverview}}
This section shall describe the contents of the document and explains how the rest of the report is organized.

%----------------------------------------------------
% TEST CONFIGURATION - baseline
%----------------------------------------------------
\section{Test Configuration \label{sect:configuration}}
This section shall contain a full identification of the hardware and the software to which this document applies, providing
the \product  baseline.\\
Additionally, this paragraph shall also document the documents, input data description, software requirements, interface requirements and/or ICDs and coding
standards that the tests use or follow.\\
Reference to where all software used and input and output data can be retrieved, if needed again, should be provided.
\subsection{Documents \label{sect:docsconf}}
Specify the documents composing the baseline for which we are testing against. Especially it is important to provide the  issue
and revision numbers of the Software Testing Specification document.
\subsection{Software \label{sect:swconf}}
Specify the characteristics and configuration of the software that was run during the execution of the tests. This may include
system software as operating systems, compilers, etc.
\subsection{Hardware \label{sect:hwconf}}
Specify the characteristics and configuration of the hardware required to execute the tests.
\subsection{Input Data \label{sect:inputdata}}
Specify the baseline of the input data used.

%--------------------------------------------------
% PERSONNEL
%--------------------------------------------------
\section{Personnel \label{sect:personnel}}
This paragraph shall specify the personnel participating in the testing activities reported in the document and their
roles or responsibilities.

\newpage

%--------------------------------------------------
% OVERVIEW OF THE TEST RESULTS
%--------------------------------------------------
\section{Overview of the Test Results \label{sect:overview}}
\subsection{Summary Table \label{sect:summarytable}}
The following table shall summarize, among others, the completion status of each test case.\\

% The table should follow this template for the traceability scripts retrieve the information correctly
\begin{longtable} {|p{0.2\textwidth}|p{0.2\textwidth}|p{0.6\textwidth}|}\hline
{\bf TEST CASE ID} & {\bf PASS/FAIL} & {\bf COMMENTS} \\\hline
DRP-00-00 & pass/fail & comment \\\hline
DRP-00-05 & pass/fail & comment \\\hline
DRP-00-10 & pass/fail & comment \\\hline
DRP-00-15 & pass/fail & comment \\\hline
DRP-00-20 & pass/fail & comment \\\hline
DRP-00-25 & pass/fail & comment \\\hline
DRP-00-30 & pass/fail & comment \\\hline
DRP-00-35 & pass/fail & comment \\\hline
\end{longtable}


\subsection{Overall Assessment \label{sect:overallassessment}}
\begin{itemize}
\item Provide an overall assessment of the software as demonstrated by the test results in this report
\item Identify any remaining deficiencies, limitations or constraints that were detected by the testing performed
\item For each remaining deficiency, limitation or constraint describe:
\begin{itemize}
\item Its impact on software and system performance, including identification of requirements not met
\item The impact on software and system design to correct it
\item A recommended solution/approach for correcting it
\item Mantis issue
\end{itemize}
\end{itemize}

\subsection{Impact of Test Environment \label{sect:impact}}
This paragraph shall provide an assessment of the manner in which the test environment may be different from the
operational environment and the effect of this difference on the test results.

\subsection{Recommended Improvements \label{sect:recommendations}}
This section shall provide any recommended improvement in the design, operation or testing of the software tested. A discussion
of each recommendation and its impact on the software may be provided.

\begin{note}
Data Facility to provide content.
\end{note}

Operational concerns
\begin{itemize}
  \item Missing handoff process from Pipelines to Operations.
  \item Generic issues
  \begin{itemize}
    \item Difficulty to keep pipeline definition up to date
    \item Using Butler
    \item Inconsistent naming
    \item Storing metadata to database
  \end{itemize}
  \item Infrastructure issues
  \begin{itemize}
    \item Uptime of cluster, problems with GPFS
    \item Storage space, compete with developer cluster use
  \end{itemize}
  \item Missing overall processing strategy
  \item Issues specific to DESDM software
  \begin{itemize}
    \item begblock scaling
    \item restart unit is per tract
    \item The cleanup of files that are no longer needed is manual and done by operator
  \end{itemize}
\end{itemize}

\newpage

\section{Detailed Test Results \label{sect:detailed}}
This section describes more in detail the results of those test cases that are failed or need a more detailed revision.
\subsection{[TEST-ID]}
\subsubsection{Summary of test results}
This paragraph shall summarize the results of the test. When the completion status of the test is not 'PASS', this section
shall reference the following paragraphs for details.

\paragraph{[DRP-00-05]}
Basic data completeness and integratiry checks from LDM-534 4.2.8.3
\begin{itemize}
  \item check the existence of the expected data
  \begin{itemize}
    \item Table listing all release data products per tract:  expected counts, actual counts, number with 0 filesizes for that datasetType
    \item Results of running truth checker per tract
  \end{itemize}
  \item Check existence metadata
  \begin{itemize}
    \item Table \ref{tab:metadata} shows the metadata we check for each data product type, such as visit, ccd, tract IDs.
\input{tableProductMetadata}
    \item Results of test (Non-null values in appropriate DB table for each file)
    \item DatasetType Expected Metadata num of files with NULLs for each Metadata
  \end{itemize}
  \item Check existence provenance ((shortcut) by attempt, OPM WAS GENERATED BY, OPM WAS USED BY)
  \begin{itemize}
    \item Verify each file can be linked with the step and processing attempt
          (Check for null pfw{\_}attempt{\_}id entry in desfile)
          (Check for null wgb{\_}id entry in desfile (and wgb{\_}id is in correct attempt)
    \item Verify information linking input files to each step was saved to Oracle
          (Check that at least 1 file of each input dataset type was tracked as input for each step based upon science description of pipeline - Bonus if we can check 1 vs > 1)
    \item (known products from each pipeline: https://confluence.lsstcorp.org/display/~hchiang2/Notes+on+existing+pipeline+components )
    \item Attach a full graph (graphml format) for the small tract?
  \end{itemize}
  \item Check (existence) runtime metrics
  \begin{itemize}
    \item For each tract, wallclock time vs exec totals vs overheads
    \item For each tract + executable, sum wallclock times, avg memory usage
  \end{itemize}
\begin{note}
Data Facility to provide content.
\end{note}
\end{itemize}

\subsubsection{Problems encountered}
\paragraph{[TEST-CASE-ID]}
This paragraph shall identify by their unique identification, the test cases in which one or more problems occurred, and shall provide:
\begin{itemize}
\item A description of the problem that occurred
\item Identification of the test procedure step in which it occurred
\item Reference to the Mantis issue
\item The number of times the procedure or step was repeated in attempting to correct the problem and the outcome of each attempt (if applicable)
\item Test steps where tests were resumed for retesting (if applicable)
\end{itemize}

\paragraph{[DRP-00-05]}
\begin{itemize}
  \item Hardware network problems (fix: replace current broken routers and purchased a hot spare)
  \item Busy cluster (fix: special reservations/queue)
  \item Filesystem full (fix: turned on purge of the scratch filesystem, purchasing production filesystems)
  \item Commas in patch IDs (a known problem with a temporary fix, ref jira tickets)
\begin{note}
Data Facility to provide content.
\end{note}
\end{itemize}

\subsubsection{Deviations from test cases/procedures}
\paragraph{[TEST-CASE-ID]}
This paragraph shall identify by their unique identification, the test cases in which one or more deviations occured and shall provide:
\begin{itemize}
\item A description of the deviation (e.g. substitution of any resource, procedural step not followed, scchedule deviation, etc)
\item The rationale of the deviation
\item An assessment of the deviation impact on the validity of the test case
\end{itemize}

\paragraph{[DRP-00-05]}
\begin{itemize}
  \item None
\begin{note}
Data Facility to provide content.
\end{note}
\end{itemize}

\newpage

\section{Test Review Board Declaration\label{sect:trb}}
This section must be filled only if the test campaign has a  formal
test review board.
After the tests are executed and the test team circulates a draft of
this document the TRB meets to declare the completion of the test campaign.
This section must contain this declaration and the pass of fail status
of the tests after completion (in case of failing it should also account
for the actions taken - Mantis issues, etc.)

\newpage

\appendix
\section{ANNEX A: Unit Tests \label{sect:unit_tests}}
Description of the unit test coverage given by Cobertura

\newpage
\section{ANNEX B: Patch releases verification}

{\it describe here the test activities executed in order to garantee the quality of the patch release}

\subsubsection{Patch release C.M.n}

Due to the small number issues fixed with the {\it \product} patch release {\it C.M.n} (very short jira list)
and due to the minimal changes applied to the software, the test campaign completed for the {\it \product} major
release $C.M$ is still valid.

All unit and integration tests are still running without problems.

Following manual verification has been done:

\begin{itemize}
\item verification 1
\item ...
\end{itemize}

No system test has been re-executed, but the fix has been verified with a specific run
on {\it Enclave??} platform.

\end{document}
